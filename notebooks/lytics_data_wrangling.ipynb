{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with Lytics Profile Data - Tools and Techniques\n",
    "\n",
    "The goal of this notebook is to present some tools and techniques that can be used to wrangle Industry Dive data. \n",
    "\n",
    "## What is Data Wrangling again?\n",
    ">Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one \"raw\" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.  Some transformation techniques include: parsing, joining, standardizing, augmenting, cleansing, and consolidating. \n",
    "\n",
    "[per wikipedia](https://en.wikipedia.org/wiki/Data_wrangling)\n",
    "\n",
    "## Bad Data in, Bad Data out\n",
    "\n",
    "![bad data in bad data out](https://cdn-images-1.medium.com/max/1200/0*YCghEemt6BtW9OZV.png \"Bad Data in Bad Data out\")\n",
    "\n",
    "Many websites contain forms in order to collect information from users for various reasons.  In our case, we have signup forms for dives that asks for information about our users like so:\n",
    "\n",
    "![signup form](../data/img/signup_form.png \"signup form\")\n",
    "\n",
    "As you can see, there are fields that are restricted to pre-defined values (e.g., Job Function), and free-form fields (e.g., Company Name) where a user can type most anything they like.  Whenever users are exposed to free-form fields, there is a possibility of bad/messy/non-standardized data making into your system.\n",
    "\n",
    "For example, here are some variants of \"IKEA\" that are present for user profiles that we have:\n",
    "\n",
    "* IKEA\n",
    "* IKEA AG\n",
    "* IKEA Belgium\n",
    "* IKEA Canada\n",
    "* IKEA Danville\n",
    "* IKEA Food\n",
    "* IKEA Home Furnishings\n",
    "* IKEA Portugal\n",
    "* IKEA USA\n",
    "* IKEA US EAST, LLC 215\n",
    "* IKEA US\n",
    "\n",
    "Without some wrangling, you would not be able to aggregate these folks properly into a single group based on company.\n",
    "\n",
    "## Lytics Profile Data\n",
    "Now, let's take a look at some Lytics profile data, which consists of all information we have about users who interact with our content.  Within this data, there are key demographic fields that can help us understand who our users are, such as:\n",
    "* first and last name\n",
    "* job title\n",
    "* email domain\n",
    "* company name\n",
    "* address\n",
    "\n",
    "The data file we are going to look at is an export of the \"All\" audience segment in Lytics.\n",
    "https://activate.getlytics.com/audiences/4cc5d612f46fb86e5cfd0c995250e60c/summary?aid=2751\n",
    "\n",
    "![All Audience segment in Lytics](../data/img/lytics_all_audience_segment.png \"All Audience segment in Lytics\")\n",
    "\n",
    "Let's start looking at this data to see how we can clean it up in order to help us create more accurate statistics about our users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company_name', 'email', 'emaildomains', 'last_name', 'first_name', 'name', 'st_profile_id', 'city', 'state', 'country', 'zip', 'lytics_segment']\n",
      "# of rows left: 684514\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#dtypes = {'company': 'str', 'company_name': 'str', 'domain': 'object', 'emaildomain': 'object', 'emaildomains': 'object',\n",
    "        # 'st_profile_id': 'object', 'user_id': np.float64, 'lytics_segment': 'object'}\n",
    "df = pd.read_csv('../data/files/lytics_profile_data_export.csv', sep=',', error_bad_lines=False, index_col=False, encoding='latin-1')\n",
    "                 #, dtype=dtypes)\n",
    "\n",
    "# list columns in dataset\n",
    "print(list(df))\n",
    "\n",
    "# number of rows\n",
    "print('# of rows left: %s' % df.shape[0])\n",
    "# print(df[df['st_profile_id'].str.contains(\"5a2ba1f6ff530ac11a8b4868\", na=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple fields in the data we can choose to cleanup, but first let's look at the \"company_name\" field.  One of the first things we should do is get rid of rows with company name values we don't care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows left: 428204\n"
     ]
    }
   ],
   "source": [
    "# remove null company name values\n",
    "df = df.dropna(subset=['company_name'])\n",
    "\n",
    "# number of rows\n",
    "print('# of rows left: %s' % df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.' '********' '..' '...' '*' '.....' '......' ',' '????????' '??????'\n",
      " '?' ',,' '**' '@@' '???']\n",
      "# of special character value rows: 97\n",
      "# of rows left: 428107\n"
     ]
    }
   ],
   "source": [
    "# find values that are any combination of special characters\n",
    "special_char_values = df['company_name'].str.contains(\"^[!@#$%^&*(),.?]*$\", na=False)\n",
    "print(df[special_char_values].company_name.unique())\n",
    "\n",
    "# number of rows\n",
    "print('# of special character value rows: %s' % df[special_char_values].shape[0])\n",
    "df = df[~special_char_values]\n",
    "\n",
    "print('# of rows left: %s' % df.shape[0])\n",
    "# print(df[df['st_profile_id'].str.contains(\"5a2ba1f6ff530ac11a8b4868\", na=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1988' '252' '1963' '1957' '1965' '1997' '1954' '1949' '123' '111' '1970'\n",
      " '1968' '1990' '1979' '1975' '1974' '1989' '1967' '1984' '104' '1959'\n",
      " '1977' '1992' '0' '900' '1986' '1993' '605' '1998' '1969' '1985' '1972'\n",
      " '1000' '5' '1948' '1999' '1953' '451' '1995' '1960' '1980' '8020' '8760'\n",
      " '1987' '1956' '53' '1996' '1991' '1973' '1994' '1958' '2714' '1955'\n",
      " '1928' '1978' '1961' '1935' '1' '1947' '43' '15' '50' '1971' '34' '1946'\n",
      " '2016' '1952' '271' '1976' '1982' '1966' '1690' '47723' '1964' '2' '1940'\n",
      " '1983' '198' '2013' '1942' '5358359981' '151' '1951' '9172077326' '1950'\n",
      " '2008' '2020' '13' '295' '6' '411' '1962' '2015' '83255804' '359' '1945'\n",
      " '2009' '136' '438' '963' '32000' '309' '59' '1981' '8001504151' '14'\n",
      " '825' '404' '68' '525' '555' '789243438' '308' '1107' '673282495' '365'\n",
      " '3211' '2040' '6164381822' '40041466109' '10' '745' '1776' '12' '61'\n",
      " '2005' '3300909815' '8121066529' '130' '173' '3' '458' '2019' '777'\n",
      " '701010' '2435500285' '352158191' '28' '983' '347' '256' '1936' '2012'\n",
      " '7' '19' '465' '929' '591' '6489' '360' '54' '207' '58' '42']\n",
      "# of number value rows: 663\n",
      "# of rows left: 427444\n"
     ]
    }
   ],
   "source": [
    "# find values that are only numbers\n",
    "number_values = df['company_name'].str.contains(\"^[0-9]*$\", na=False)\n",
    "print(df[number_values].company_name.unique())\n",
    "\n",
    "# number of rows\n",
    "print('# of number value rows: %s' % df[number_values].shape[0])\n",
    "df = df[~number_values]\n",
    "\n",
    "print('# of rows left: %s' % df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows left: 427432\n"
     ]
    }
   ],
   "source": [
    "# random additional values that I found when I was looking at the data in Excel\n",
    "weird_vals = ['#NAME?', '{Re}', '< self >']\n",
    "weird_values = df['company_name'].isin(weird_vals)\n",
    "df = df[~weird_values]\n",
    "\n",
    "# left over rows in dataframe\n",
    "print('# of rows left: %s' % df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned all the bad company name values from our dataset, let's work on standardizing the names to help with comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows left: 426539\n"
     ]
    }
   ],
   "source": [
    "# change the values to all lower case\n",
    "df['stndrdzed_company_name'] = df['company_name'].str.lower()\n",
    "# remove all punctuation\n",
    "df[\"stndrdzed_company_name\"] = df['stndrdzed_company_name'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# remove rows with \"none\" as value\n",
    "none_rows = df['stndrdzed_company_name'].str.contains('none', na=False)\n",
    "df = df[~none_rows]\n",
    "\n",
    "# remove rows with \"\" as value\n",
    "empty_string_rows = df['stndrdzed_company_name'].values == ''\n",
    "df = df[~empty_string_rows]\n",
    "print('# of rows left: %s' % df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our dataset to see what we are working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stndrdzed_company_name</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>20160506deleteme</td>\n",
       "      <td>1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202602</th>\n",
       "      <td>self</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109066</th>\n",
       "      <td>ibm</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248957</th>\n",
       "      <td>walmart</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3477</th>\n",
       "      <td>accenture</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138980</th>\n",
       "      <td>macys</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217753</th>\n",
       "      <td>student</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68501</th>\n",
       "      <td>duke energy</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154431</th>\n",
       "      <td>mr</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202620</th>\n",
       "      <td>self employed</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222432</th>\n",
       "      <td>target</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164095</th>\n",
       "      <td>novartis</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249612</th>\n",
       "      <td>waste management</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62047</th>\n",
       "      <td>deloitte</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94800</th>\n",
       "      <td>google</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185124</th>\n",
       "      <td>pwc</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53173</th>\n",
       "      <td>consultant</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191684</th>\n",
       "      <td>retired</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11577</th>\n",
       "      <td>amazon</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190937</th>\n",
       "      <td>republic services</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256487</th>\n",
       "      <td>xcel energy</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211935</th>\n",
       "      <td>southern california edison</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176157</th>\n",
       "      <td>pfizer</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157532</th>\n",
       "      <td>national grid</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744</th>\n",
       "      <td>amgen</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79858</th>\n",
       "      <td>ey</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87082</th>\n",
       "      <td>freelance</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198999</th>\n",
       "      <td>sanofi</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175261</th>\n",
       "      <td>pepsico</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12057</th>\n",
       "      <td>american electric power</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94125</th>\n",
       "      <td>golamago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94123</th>\n",
       "      <td>gokosher</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94094</th>\n",
       "      <td>goiko autosa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94122</th>\n",
       "      <td>gokin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94095</th>\n",
       "      <td>goin my way travel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94096</th>\n",
       "      <td>goincka ltd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94097</th>\n",
       "      <td>going for green inc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94098</th>\n",
       "      <td>going green publications</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94099</th>\n",
       "      <td>going green recycle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94100</th>\n",
       "      <td>going mobile  llc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94101</th>\n",
       "      <td>going up</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94102</th>\n",
       "      <td>going4gold global</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94103</th>\n",
       "      <td>going4it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94104</th>\n",
       "      <td>goinggreen  enviroclean</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94105</th>\n",
       "      <td>gointernational</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94106</th>\n",
       "      <td>goip aula ltd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94108</th>\n",
       "      <td>goja</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94109</th>\n",
       "      <td>gojdwingroup</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94110</th>\n",
       "      <td>gojek indonesia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94111</th>\n",
       "      <td>gojhn ltd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94112</th>\n",
       "      <td>goji</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94114</th>\n",
       "      <td>gojo industries inc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94115</th>\n",
       "      <td>gokaldas images pvt ltd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94116</th>\n",
       "      <td>gokaldas images usa inc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94117</th>\n",
       "      <td>gokart</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94118</th>\n",
       "      <td>gokart labs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94119</th>\n",
       "      <td>gokceli insaat muteahhitlikmahir coban tepe mah</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94120</th>\n",
       "      <td>goken america</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94121</th>\n",
       "      <td>goken america llc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129924</th>\n",
       "      <td>lagrange college</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>259848 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 stndrdzed_company_name  counts\n",
       "416                                    20160506deleteme    1302\n",
       "202602                                             self     644\n",
       "109066                                              ibm     510\n",
       "248957                                          walmart     495\n",
       "3477                                          accenture     483\n",
       "138980                                            macys     423\n",
       "217753                                          student     415\n",
       "68501                                       duke energy     404\n",
       "154431                                               mr     369\n",
       "202620                                    self employed     365\n",
       "222432                                           target     364\n",
       "164095                                         novartis     340\n",
       "249612                                 waste management     336\n",
       "62047                                          deloitte     307\n",
       "94800                                            google     307\n",
       "185124                                              pwc     307\n",
       "53173                                        consultant     303\n",
       "191684                                          retired     298\n",
       "11577                                            amazon     297\n",
       "190937                                republic services     296\n",
       "256487                                      xcel energy     287\n",
       "211935                       southern california edison     286\n",
       "176157                                           pfizer     279\n",
       "157532                                    national grid     272\n",
       "12744                                             amgen     270\n",
       "79858                                                ey     260\n",
       "87082                                         freelance     256\n",
       "198999                                           sanofi     253\n",
       "175261                                          pepsico     248\n",
       "12057                           american electric power     248\n",
       "...                                                 ...     ...\n",
       "94125                                          golamago       1\n",
       "94123                                          gokosher       1\n",
       "94094                                      goiko autosa       1\n",
       "94122                                             gokin       1\n",
       "94095                                goin my way travel       1\n",
       "94096                                       goincka ltd       1\n",
       "94097                               going for green inc       1\n",
       "94098                          going green publications       1\n",
       "94099                               going green recycle       1\n",
       "94100                                 going mobile  llc       1\n",
       "94101                                          going up       1\n",
       "94102                                 going4gold global       1\n",
       "94103                                          going4it       1\n",
       "94104                           goinggreen  enviroclean       1\n",
       "94105                                   gointernational       1\n",
       "94106                                     goip aula ltd       1\n",
       "94108                                              goja       1\n",
       "94109                                      gojdwingroup       1\n",
       "94110                                   gojek indonesia       1\n",
       "94111                                         gojhn ltd       1\n",
       "94112                                              goji       1\n",
       "94114                               gojo industries inc       1\n",
       "94115                           gokaldas images pvt ltd       1\n",
       "94116                           gokaldas images usa inc       1\n",
       "94117                                            gokart       1\n",
       "94118                                       gokart labs       1\n",
       "94119   gokceli insaat muteahhitlikmahir coban tepe mah       1\n",
       "94120                                     goken america       1\n",
       "94121                                 goken america llc       1\n",
       "129924                                 lagrange college       1\n",
       "\n",
       "[259848 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby('stndrdzed_company_name')\n",
    "\n",
    "grouped = grouped.size().reset_index(name='counts')\n",
    "grouped.sort_values(by=['counts'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note from looking at this is that there are company names that contain values other than English.  For instance, \"현대엔지니어링\" is Korean.  This is one thing you could work on eliminating as well if you wanted to focus on English values.  I tried to use a library called \"langdetect\" for this, but it did not do a good job of picking up the obvious cases.\n",
    "\n",
    "Once we have wrangled the data bit, we can now try to enhance our dataset with an external dataset.  One of the datasets we bought rights to recently, DiscoverOrg, has different information about companies that could be useful for analysis.  The common field these two datasets have is the company name.  So we can try to load this dataset, clean it up a bit, then compare it to our original cleaned dataset in order to try and match on company name and enhance our existing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes= {'Company ID': np.int64, 'Company Name': 'str', 'Company Website': 'object', 'Company HQ Phone': 'object',\n",
    "        'Company Email Domain': 'object', 'Company Description': 'object', 'Company Primary Industry': 'object',\n",
    "        'Company Revenue': np.float64, 'Company IT Budget (Mil)': 'object', 'Number of Employees': np.int64,\n",
    "        'Company IT Employees': np.float64, 'Company Fortune Rank': np.float64, 'Company Ownership': 'object', 'Company Profile URL': 'object',\n",
    "        'Company Business Model (B2B/B2C/B2G)': 'object', 'Hospital Beds': 'object', 'HQ Address 1': 'object', 'HQ Address 2': 'object',\n",
    "        'HQ City': 'object', 'HQ State': 'object', 'HQ Postal Code': 'object', 'HQ County': 'object', 'HQ Country': 'object'\n",
    "        }\n",
    "df2 = pd.read_csv('../data/files/DiscoverOrg_Company_223030_20180731141156.csv', encoding='latin-1', sep=',', error_bad_lines=False, index_col=False, dtype=dtypes)\n",
    "\n",
    "# change the values to all lower case\n",
    "df2['stndrdzed_company_name'] = df2['Company Name'].astype(str).str.lower()\n",
    "# remove all punctuation\n",
    "df2[\"stndrdzed_company_name\"] = df2['stndrdzed_company_name'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows left: 92940\n"
     ]
    }
   ],
   "source": [
    "# merge with discovery org data in order to find matches\n",
    "merged_rows = pd.merge(df, df2, how= 'left', on= 'stndrdzed_company_name', sort=True, suffixes=('_a', '_b'),)\n",
    "\n",
    "# number of rows after merging\n",
    "print('# of rows left: %s' % merged_rows.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, after our intial cleaning process, we had 456,521 rows in our lytics file.  Our DiscoverOrg file had 68,735 rows.  By merging the two files on company name we were able to match 98,811 rows.  That is not a bad start.\n",
    "\n",
    "Next, we will write the merged and non-merged rows to a file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path=r'/Users/sasanbahadaran/Downloads'\n",
    "merged_rows.to_csv(os.path.join(path,r'lytics_profile_disc_org_merged_rows.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can work on wrangling other fields and joining them to the DiscoveryOrg data for enhancement, such as:\n",
    "* email domain\n",
    "* address\n",
    "\n",
    "We could also decide to work on further cleaning up the company data as well.  This could be through performing additional wrangling after examining our output file, or going beyond the deterministic types of methods we have covered so far.  All in all though, the more we standardize our dataset, the better results we will get when performing analysis on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas: Take address, standardize and clean, match up to discOrg and see what additional results you can yield.  Take email domain and do the same.  Filter out more junk based on analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
