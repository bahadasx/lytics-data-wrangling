{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with Lytics Profile Data - Tools and Techniques\n",
    "\n",
    "The goal of this notebook is to present some tools and techniques that can be used to wrangle Industry Dive data. \n",
    "\n",
    "## What is Data Wrangling again?\n",
    ">Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one \"raw\" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.  Some transformation techniques include: parsing, joining, standardizing, augmenting, cleansing, and consolidating. \n",
    "\n",
    "[per wikipedia](https://en.wikipedia.org/wiki/Data_wrangling)\n",
    "\n",
    "## Bad Data in, Bad Data out\n",
    "\n",
    "![bad data in bad data out](https://cdn-images-1.medium.com/max/1200/0*YCghEemt6BtW9OZV.png \"Bad Data in Bad Data out\")\n",
    "\n",
    "Many websites contain forms in order to collect information from users for various reasons.  In our case, we have signup forms for dives that asks for information about our users like so:\n",
    "\n",
    "![signup form](../data/img/signup_form.png \"signup form\")\n",
    "\n",
    "As you can see, there are fields that are restricted to pre-defined values (e.g., Job Function), and free-form fields (e.g., Company Name) where a user can type most anything they like.  Whenever users are exposed to free-form fields, there is a possibility of bad/messy/non-standardized data making into your system.\n",
    "\n",
    "For example, here are some variants of \"IKEA\" that are present for user profiles that we have:\n",
    "\n",
    "* IKEA\n",
    "* IKEA AG\n",
    "* IKEA Belgium\n",
    "* IKEA Canada\n",
    "* IKEA Danville\n",
    "* IKEA Food\n",
    "* IKEA Home Furnishings\n",
    "* IKEA Portugal\n",
    "* IKEA USA\n",
    "* IKEA US EAST, LLC 215\n",
    "* IKEA US\n",
    "\n",
    "Without some wrangling, you would not be able to aggregate these folks properly into a single group based on company.\n",
    "\n",
    "## Lytics Profile Data\n",
    "We now use Lytics in order to house all data we know about users who interact with our content.  This data comes from many systems, but regardless of source, there are certain demographic fields in this dataset that can help us understand who our users are, such as:\n",
    "* first and last name\n",
    "* job title\n",
    "* email domain\n",
    "* company name\n",
    "* address\n",
    "\n",
    "The data file being used for this notebook is an export of the \"All\" audience segment in Lytics.\n",
    "https://activate.getlytics.com/audiences/4cc5d612f46fb86e5cfd0c995250e60c/summary?aid=2751\n",
    "\n",
    "![All Audience segment in Lytics](../data/img/lytics_all_audience_segment.png \"All Audience segment in Lytics\")\n",
    "\n",
    "First, we will load our data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company', 'company_name', 'domain', 'emaildomain', 'emaildomains', 'st_profile_id', 'user_id', 'lytics_segment']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "782425"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/files/lytics_profile_data_export.csv', encoding='latin-1')\n",
    "\n",
    "# list columns in dataset\n",
    "print(list(df))\n",
    "\n",
    "# number of rows\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple fields in the data we can choose to cleanup, but first let's look at the \"company_name\" field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458289"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove null company name values\n",
    "df = df.dropna(subset=['company_name'])\n",
    "\n",
    "# number of rows\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..' '.' '...' '*' '********' '......' ',' '.....' '***' '????????' '?'\n",
      " '**' '.......' ',,' '@@']\n",
      "458289\n"
     ]
    }
   ],
   "source": [
    "# find values that are any combination of special characters\n",
    "special_char_values = df['company_name'].str.contains(\"^[!@#$%^&*(),.?]*$\", na=False)\n",
    "print(df[special_char_values].company_name.unique())\n",
    "# number of rows\n",
    "print(df.shape[0])\n",
    "df = df[~special_char_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1948' '1989' '1954' '451' '1957' '1979' '252' '1953' '1967' '8020'\n",
      " '1960' '5' '104' '1999' '123' '1974' '1988' '1977' '1000' '900' '1956'\n",
      " '605' '8760' '1984' '1959' '1998' '1972' '1992' '1997' '1991' '111'\n",
      " '1990' '1987' '1970' '1969' '1965' '1968' '1995' '1993' '1975' '1963'\n",
      " '231112027' '53' '1976' '1985' '1949' '149' '0' '1971' '1986' '346'\n",
      " '47723' '1947' '94122202312' '1' '1958' '1973' '43' '1935' '1961' '1994'\n",
      " '1946' '325024080134' '1996' '1982' '15' '34' '1952' '271' '1980' '1966'\n",
      " '1936' '47' '1978' '1964' '1928' '50' '2714' '1955' '1690' '1942' '13'\n",
      " '05358359981' '9172077326' '12' '151' '1951' '2000' '400000000000' '2'\n",
      " '1905' '2020' '1940' '1983' '2008' '198' '2013' '1962' '411' '2015' '295'\n",
      " '1950' '940005848995' '11455' '83255804' '2166833' '1001' '6' '91957'\n",
      " '14' '887000000000' '666' '59' '963' '32000' '555' '404' '0789243438'\n",
      " '438' '68' '1945' '525' '825' '2009' '1981' '8001504151' '136' '359'\n",
      " '365' '308' '940003979987' '6164381822' '1107' '0673282495' '2040' '745'\n",
      " '3211' '1871' '40041466109' '10' '1776' '1931' '2005' '8121066529'\n",
      " '42149777' '173' '130' '3' '3300909815' '458' '2019' '352158191' '347'\n",
      " '777' '701010' '002435500285' '983' '256' '940004104374' '918015150236'\n",
      " '28' '929' '591' '19' '2012' '719' '7' '465' '42' '207' '360' '2016' '54'\n",
      " '6489' '58']\n",
      "721\n"
     ]
    }
   ],
   "source": [
    "# find values that are only numbers\n",
    "number_values = df['company_name'].str.contains(\"^[0-9]*$\", na=False)\n",
    "print(df[number_values].company_name.unique())\n",
    "# number of rows\n",
    "print(df[number_values].shape[0])\n",
    "df = df[~number_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457462\n"
     ]
    }
   ],
   "source": [
    "# random additional values that I found when I was looking at the data in Excel\n",
    "weird_vals = ['#NAME?', '{Re}', '< self >']\n",
    "weird_values = df['company_name'].isin(weird_vals)\n",
    "df = df[~weird_values]\n",
    "# left over rows in dataframe\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-311-d834e9ce6ef2>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-311-d834e9ce6ef2>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    First, we will look at some techniques to apply to our dataset based on a recent request from Audience Dev.  They would like to create aggregate statistics about our users based on company name, so this will be the basis upon which we will transform our data.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Use Cases\n",
    "\n",
    "### Company Name\n",
    "First, we will look at some techniques to apply to our dataset based on a recent request from Audience Dev.  They would like to create aggregate statistics about our users based on company name, so this will be the basis upon which we will transform our data.\n",
    "\n",
    "That lytics export file (3,488,529 rows/50.7 MB) made my RAM unhappy, so I decided to cut the file down based on the above-stated use case.  \n",
    "\n",
    "\n",
    "First, I removed all rows from the file which had a blank Company Name.  Next, I removed some obvious bad data (e.g., \"*\", \"11\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disc_org = pd.read_csv('DiscoverOrg_Company_223030_20180731141156.csv', encoding='latin-1')\n",
    "df_disc_org.columns = ['company_id', 'company_name', 'domain','company_primary_industry','hq_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disc_org.columns = ['company_id', 'company_name', 'domain','company_primary_industry','hq_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with discovery org data in order to find matches\n",
    "merge = pd.merge(df_lytics, df_disc_org, how='inner', on=['company_name'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
